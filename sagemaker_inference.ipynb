{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/kavyapundir/CustomerChurnPrediction/main/customer-retention-model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker \n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()  \n",
    "default_bucket = sagemaker_session.default_bucket() \n",
    "s3_client = boto3.resource('s3') \n",
    "s3_client.Bucket(default_bucket).upload_file(\"customer-retention-model.tar.gz\",\"model_artifacts/customer-retention-model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "model_package_group_name = f\"ChurnModelPackageGroupV2\"\n",
    "region = sagemaker_session.boto_region_name\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_package_group_input_dict = {\n",
    " \"ModelPackageGroupName\" : model_package_group_name,\n",
    "}\n",
    "\n",
    "create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "print('ModelPackageGroup Arn : {}'.format(create_model_package_group_response['ModelPackageGroupArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the image uri used to train model\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "# Specify the model source\n",
    "model_url = f\"s3://{default_bucket}/model_artifacts/customer-retention-model.tar.gz\"\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": image_uri,\n",
    "            \"ModelDataUrl\": model_url\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "      \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "   }\n",
    " }\n",
    "\n",
    "# Alternatively, you can specify the model source like this:\n",
    "# modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]=model_url\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : \"Model to detect 3 different types of irises (Setosa, Versicolour, and Virginica)\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import pandas as pd \n",
    "import sagemaker \n",
    "from sagemaker.workflow.pipeline_context import PipelineSession \n",
    "\n",
    "s3_client = boto3.resource('s3') \n",
    "pipeline_name = f\"sagemaker-mlops-inference-pipeline\" \n",
    "sagemaker_session = sagemaker.session.Session() \n",
    "region = sagemaker_session.boto_region_name \n",
    "role = sagemaker.get_execution_role() \n",
    "pipeline_session = PipelineSession() \n",
    "default_bucket = sagemaker_session.default_bucket() \n",
    "model_package_group_name = f\"ChurnModelPackageGroup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ( \n",
    " ParameterInteger, \n",
    " ParameterString, \n",
    " ParameterFloat) \n",
    "\n",
    "base_job_prefix = \"churn-example\"\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString( name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "transform_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "transform_instance_count = ParameterInteger(name=\"TransformInstanceCount\", default_value=1)\n",
    "batch_data_path = \"s3://{}/data/batch/batch.csv\".format(default_bucket)\n",
    "model_approval_status = ParameterString( name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    ## Convert to datetime columns\n",
    "    df[\"firstorder\"]=pd.to_datetime(df[\"firstorder\"],errors='coerce')\n",
    "    df[\"lastorder\"] = pd.to_datetime(df[\"lastorder\"],errors='coerce')\n",
    "    ## Drop Rows with null values\n",
    "    df = df.dropna()\n",
    "    ## Create Column which gives the days between the last order and the first order\n",
    "    df[\"first_last_days_diff\"] = (df['lastorder']-df['firstorder']).dt.days\n",
    "    ## Create Column which gives the days between when the customer record was created and the first order\n",
    "    df['created'] = pd.to_datetime(df['created'])\n",
    "    df['created_first_days_diff']=(df['created']-df['firstorder']).dt.days\n",
    "    ## Drop Columns\n",
    "    df.drop(['custid','created','firstorder','lastorder'],axis=1,inplace=True)\n",
    "    ## Apply one hot encoding on favday and city columns\n",
    "    df = pd.get_dummies(df,prefix=['favday','city'],columns=['favday','city'])\n",
    "    return df\n",
    "    \n",
    "\n",
    "store_data = pd.read_csv(\"storedata_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess batch data and save into the data folder\n",
    "batch_data = preprocess_batch_data(\"storedata_total.csv\")\n",
    "batch_data.pop(\"retained\")\n",
    "batch_sample = batch_data.sample(frac=0.2)\n",
    "pd.DataFrame(batch_sample).to_csv(\"batch.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.Bucket(default_bucket).upload_file(\"batch.csv\",\"data/batch/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\") \n",
    "\n",
    "# get a list of approved model packages from the model package group you specified earlier\n",
    "approved_model_packages = sm_client.list_model_packages(\n",
    "      ModelApprovalStatus='Approved',\n",
    "      ModelPackageGroupName=model_package_group_name,\n",
    "      SortBy='CreationTime',\n",
    "      SortOrder='Descending'\n",
    "  )\n",
    "\n",
    "# find the latest approved model package\n",
    "try:\n",
    "    latest_approved_model_package_arn = approved_model_packages['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "except Exception as e:\n",
    "    print(\"Failed to retrieve an approved model package:\", e)\n",
    "    \n",
    "print(latest_approved_model_package_arn) \n",
    "\n",
    " # retrieve required information about the model\n",
    "latest_approved_model_package_descr =  sm_client.describe_model_package(ModelPackageName = latest_approved_model_package_arn)\n",
    "\n",
    "# model artifact uri (tar.gz file)\n",
    "model_artifact_uri = latest_approved_model_package_descr['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "# sagemaker image in ecr\n",
    "image_uri = latest_approved_model_package_descr['InferenceSpecification']['Containers'][0]['Image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model = Model(\n",
    "image_uri=image_uri,\n",
    "model_data=model_artifact_uri,\n",
    "sagemaker_session=pipeline_session,\n",
    "role=role\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "name=\"ChurnCreateModel\",\n",
    "step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/ChurnTransform\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "                                 \n",
    "step_transform = TransformStep(\n",
    "    name=\"ChurnTransform\", \n",
    "    step_args=transformer.transform(\n",
    "                    data=batch_data_path,\n",
    "                    content_type=\"text/csv\"\n",
    "                 )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        transform_instance_type,\n",
    "        transform_instance_count,\n",
    "        batch_data,\n",
    "    ],\n",
    "    steps=[step_create_model,step_transform],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "# start Pipeline execution\n",
    "pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
